{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLABORRATIVE FILTERING FOR THE MOVIELENS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build two models <br>\n",
    "1. A Simple Dot product model that illustrates the construction and use of latent factors, we will try and optimise that model with gradient decent\n",
    "2. Then we will define a simple Neural Network and fit it to the data and compare it with the Academic state of the Art for this problem.<br>\n",
    "I am using a smaller 100,000 dataset to make things faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would normally do this with a utility file that woud have all my helper functions but in this case doing it here so you can see all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "import math, os, json, sys, re\n",
    "import  pickle\n",
    "from importlib import reload\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy.random import random, permutation, randn, normal, uniform, choice\n",
    "from numpy import newaxis\n",
    "import scipy\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy.ndimage import imread\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import bcolz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, activity_l2, l1, activity_l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING & DATA SETUP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Movielens data \n",
    "- Small: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users\n",
    "- Large: 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build out or model on the smaller dataset and look at the results we get.\n",
    "Also define a folder where we can store the models that we define.\n",
    "We also define a hyperpaprameter batch_size - this depends on the size that my GPU can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"data/ml-20m/\"\n",
    "path = \"./data/ml-latest-small/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the data in using pandas and the *read_csv* mathod and take a look at the first five entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(path+'ratings.csv')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings) # get the total number of ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are also using the other files that has the name of the movies so we can use it to display some results later.\n",
    "- We use the movieId as the key and the names as the values of the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toy Story (1995)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now get all the users and the movies in the ratings list & since each row is a single user rating a single movie with a rating we wil use the unique method to get the arrays for the users and the movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings.userId.unique()\n",
    "movies = ratings.movieId.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now build out the userid to index and the movie id to index so we can look them up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid2idx = {o:i for i,o in enumerate(users)}\n",
    "movieid2idx = {o:i for i,o in enumerate(movies)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now get the movieId in the ratings dataframe to be the same as the one in the movieid2idx by looking it up and applying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.movieId = ratings.movieId.apply(lambda x: movieid2idx[x])\n",
    "ratings.userId = ratings.userId.apply(lambda x: userid2idx[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take a look at some of the ranges of data that we have in the ratings table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 609, 0, 9723)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_min, user_max, movie_min, movie_max = (ratings.userId.min(), \n",
    "    ratings.userId.max(), ratings.movieId.min(), ratings.movieId.max())\n",
    "user_min, user_max, movie_min, movie_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the number of unique users and the total number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = ratings.userId.nunique()\n",
    "n_movies = ratings.movieId.nunique()\n",
    "n_users, n_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we using collaborative filtering and supposing there are latent factors for the kinds of users and the kinds of movies we have to decide how many latent factor we want to use as influencing both. \n",
    "I picked it to be 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the seed for repetability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the training data into training set and validation set, i have decided it to be a 80-20 split.\n",
    "We get some ratings for all users and some ratings for all movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(ratings)) < 0.8\n",
    "trn = ratings[msk]\n",
    "val = ratings[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOT PRODUCT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first just create a dot product model where take the matrix dot product of the latent factors for both users and the movies and try and minimize the ratings error.\n",
    "\n",
    "- We are going to random initialize the latent factors and use gradient decent to learn them\n",
    "- We are using the \"Embedding\" layer of Keras - which convenietly looks up the latent factor for each user and movie and saves us the implementation of the one hot encoding matrix multiplication which could be slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an Input layer for both the users and the movies.<br>\n",
    "Creating the Embedding layers with l2 weight regularization for both the users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/veeville/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1029: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users, n_factors, input_length=1, W_regularizer=l2(1e-4))(user_in)\n",
    "\n",
    "movie_in = Input(shape=(1,), dtype='int64', name='movie_in')\n",
    "m = Embedding(n_movies, n_factors, input_length=1, W_regularizer=l2(1e-4))(movie_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the dot product of the embed layers and define the model.<br>\n",
    "Compile the model with a learing rate of 0.001 and train it for just 1 epochs first<br>\n",
    "Using the mean sqaured error loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/veeville/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1047: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/veeville/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1108: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "x = merge([u, m], mode='dot')\n",
    "x = Flatten()(x)\n",
    "model = Model([user_in, movie_in], x)\n",
    "model.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/1\n",
      "80727/80727 [==============================] - 5s - loss: 9.6342 - val_loss: 4.1563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d0f22be0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=1, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some learing rate Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/3\n",
      "80727/80727 [==============================] - 4s - loss: 3.0259 - val_loss: 2.7898\n",
      "Epoch 2/3\n",
      "80727/80727 [==============================] - 4s - loss: 2.3241 - val_loss: 2.5861\n",
      "Epoch 3/3\n",
      "80727/80727 [==============================] - 4s - loss: 2.1547 - val_loss: 2.5496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea98446240>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=3, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/6\n",
      "80727/80727 [==============================] - 4s - loss: 2.0902 - val_loss: 2.5351\n",
      "Epoch 2/6\n",
      "80727/80727 [==============================] - 4s - loss: 2.0545 - val_loss: 2.5379\n",
      "Epoch 3/6\n",
      "80727/80727 [==============================] - 4s - loss: 2.0261 - val_loss: 2.5437\n",
      "Epoch 4/6\n",
      "80727/80727 [==============================] - 4s - loss: 2.0046 - val_loss: 2.5424\n",
      "Epoch 5/6\n",
      "80727/80727 [==============================] - 4s - loss: 1.9817 - val_loss: 2.5558\n",
      "Epoch 6/6\n",
      "80727/80727 [==============================] - 4s - loss: 1.9595 - val_loss: 2.5674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d1ca8e80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=6, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING BIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model is missing some parts here - there are users who are movie buffs and rate most movies and there are movies that most people like to account for this we add a user & movie bias to our model and train again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a helper function that creates the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_in, u = create_embed('user_in', n_users, n_factors, 1e-4)\n",
    "movie_in, m = create_embed('movie_in', n_movies, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another helper that creates a bias with an embedding with a single output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the user bias & the movie bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub = create_bias(user_in, n_users)\n",
    "mb = create_bias(movie_in, n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = merge([u, m], mode='dot')\n",
    "x = Flatten()(x)\n",
    "x = merge([x, ub], mode='sum')\n",
    "x = merge([x, mb], mode='sum')\n",
    "model = Model([user_in, movie_in], x)\n",
    "model.compile(Adam(0.01), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/1\n",
      "80727/80727 [==============================] - 5s - loss: 3.4501 - val_loss: 1.8945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d02f3630>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=1, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some Learning Rate Annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/6\n",
      "80727/80727 [==============================] - 5s - loss: 1.5164 - val_loss: 1.3997\n",
      "Epoch 2/6\n",
      "80727/80727 [==============================] - 5s - loss: 1.1105 - val_loss: 1.1651\n",
      "Epoch 3/6\n",
      "80727/80727 [==============================] - 5s - loss: 0.9657 - val_loss: 1.1317\n",
      "Epoch 4/6\n",
      "80727/80727 [==============================] - 5s - loss: 0.9327 - val_loss: 1.1314\n",
      "Epoch 5/6\n",
      "80727/80727 [==============================] - 5s - loss: 0.9197 - val_loss: 1.1420\n",
      "Epoch 6/6\n",
      "80727/80727 [==============================] - 5s - loss: 0.9156 - val_loss: 1.1432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d041ab00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=6, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.9100 - val_loss: 1.1447\n",
      "Epoch 2/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.9012 - val_loss: 1.1501\n",
      "Epoch 3/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8990 - val_loss: 1.1496\n",
      "Epoch 4/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8975 - val_loss: 1.1531\n",
      "Epoch 5/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8934 - val_loss: 1.1628\n",
      "Epoch 6/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8935 - val_loss: 1.1619\n",
      "Epoch 7/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8901 - val_loss: 1.1555\n",
      "Epoch 8/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8894 - val_loss: 1.1513\n",
      "Epoch 9/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8841 - val_loss: 1.1547\n",
      "Epoch 10/10\n",
      "80727/80727 [==============================] - 5s - loss: 0.8896 - val_loss: 1.1484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea3eb04588>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.userId, trn.movieId], trn.rating, batch_size=64, nb_epoch=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO that took us to a Vaidation loss of about 1.1 that is not very great since the Academic state of the art for such a problem for this dataset RMSE is 0.89. So we are not there yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving the Model & Reloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'bias.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'bias.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Prediction for user #3 and movie #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.0126]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([np.array([3]), np.array([6])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a Neural net model by concatenating the user and movie vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_in, u = create_embed('user_in', n_users, n_factors, 1e-4)\n",
    "movie_in, m = create_embed('movie_in', n_movies, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = merge([u, m], mode='concat')\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(70, activation='relu')(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Dense(1)(x)\n",
    "nn = Model([user_in, movie_in], x)\n",
    "nn.compile(Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/10\n",
      "80727/80727 [==============================] - 3s - loss: 3.0292 - val_loss: 0.9072\n",
      "Epoch 2/10\n",
      "80727/80727 [==============================] - 3s - loss: 1.6224 - val_loss: 0.8685\n",
      "Epoch 3/10\n",
      "80727/80727 [==============================] - 3s - loss: 1.4561 - val_loss: 0.8643\n",
      "Epoch 4/10\n",
      "80727/80727 [==============================] - 3s - loss: 1.3077 - val_loss: 0.8482\n",
      "Epoch 5/10\n",
      "80727/80727 [==============================] - 3s - loss: 1.1928 - val_loss: 0.8366\n",
      "Epoch 6/10\n",
      "80727/80727 [==============================] - 3s - loss: 1.0740 - val_loss: 0.8423\n",
      "Epoch 7/10\n",
      "80727/80727 [==============================] - 3s - loss: 0.9753 - val_loss: 0.8264\n",
      "Epoch 8/10\n",
      "80727/80727 [==============================] - 3s - loss: 0.8928 - val_loss: 0.8197\n",
      "Epoch 9/10\n",
      "80727/80727 [==============================] - 3s - loss: 0.8371 - val_loss: 0.8104\n",
      "Epoch 10/10\n",
      "80727/80727 [==============================] - 3s - loss: 0.7923 - val_loss: 0.8079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9c8533a20>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit([trn.userId, trn.movieId], trn.rating, batch_size=128, nb_epoch=10, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80727 samples, validate on 20109 samples\n",
      "Epoch 1/5\n",
      "80727/80727 [==============================] - 3s - loss: 0.7682 - val_loss: 0.8070\n",
      "Epoch 2/5\n",
      "80727/80727 [==============================] - 3s - loss: 0.7517 - val_loss: 0.8074\n",
      "Epoch 3/5\n",
      "80727/80727 [==============================] - 3s - loss: 0.7421 - val_loss: 0.8091\n",
      "Epoch 4/5\n",
      "80727/80727 [==============================] - 3s - loss: 0.7410 - val_loss: 0.8106\n",
      "Epoch 5/5\n",
      "80727/80727 [==============================] - 3s - loss: 0.7385 - val_loss: 0.8114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d0589438>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit([trn.userId, trn.movieId], trn.rating, batch_size=128, nb_epoch=5, \n",
    "          validation_data=([val.userId, val.movieId], val.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better than the Academic State of Art Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving the Model & Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save_weights(model_path+'nn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.load_weights(model_path+'nn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the model get predictions for a user movie pair passing a user id and a movie id <br> -\n",
    "In this case we pass in the user #4 and movie #6 to get if the user would like the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9541526]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict([np.array([4]), np.array([6])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
